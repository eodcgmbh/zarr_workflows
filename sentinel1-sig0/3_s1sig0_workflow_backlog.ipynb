{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "028c77b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from hera.workflows import models, CronWorkflow, script, Artifact, Parameter, DAG, Steps, Step, NoneArchiveStrategy, Workflow\n",
    "from hera.shared import global_config\n",
    "\n",
    "load_dotenv(\"/home/otto/s1_zarr/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "25601f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_config.host = \"https://services.eodc.eu/workflows/\"\n",
    "global_config.namespace = \"inca\"\n",
    "global_config.token = os.getenv(\"argo_token_prod\")\n",
    "global_config.image = \"ghcr.io/oscipal/image_zarr:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ed864265",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfs_volume = [models.Volume(\n",
    "    name=\"eodc-mount\",\n",
    "    persistent_volume_claim={\"claimName\": \"eodc-nfs-claim\"},\n",
    "    )]\n",
    "\n",
    "security_context = {\"runAsUser\": 59100,\n",
    "                    \"runAsGroup\": 59100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2536a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "@script(volume_mounts=[models.VolumeMount(name=\"eodc-mount\", mount_path=\"/eodc\")])\n",
    "\n",
    "def extend_time_dimension(store_path: str = \"/eodc/private/tempearth/s1sig0.zarr\"):\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "    import zarr\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    now_np = np.datetime64(now).astype('datetime64[D]')\n",
    "    origin = np.datetime64(\"2014-10-01\")\n",
    "\n",
    "    new_shape = int((now_np-origin).astype(int))\n",
    "    new_extent = np.arange(0,new_shape,1)\n",
    "\n",
    "    store = zarr.storage.LocalStore(store_path)\n",
    "    group = zarr.group(store=store)\n",
    "\n",
    "    array_names=set(group.array_keys())\n",
    "    coords = {\"time\", \"x\", \"y\", \"relative_orbit_number\"}\n",
    "    data_arrays = array_names-coords\n",
    "\n",
    "    group[\"time\"].resize(new_shape)\n",
    "    for array in data_arrays:\n",
    "        group_shape  = group[array].shape\n",
    "        group[array].resize((group_shape[0], new_shape, group_shape[2], group_shape[3]))\n",
    "\n",
    "    zarr.consolidate_metadata(store)\n",
    "    store = zarr.storage.LocalStore(store_path)\n",
    "    group = zarr.group(store=store)\n",
    "\n",
    "    group[\"time\"][:]=new_extent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c477bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@script(volume_mounts=[models.VolumeMount(name=\"eodc-mount\", mount_path=\"/eodc\")])\n",
    "\n",
    "def write_data(tile: str, store_path: str = \"/eodc/private/tempearth/s1sig0.zarr\"):\n",
    "    print(\"test1\")\n",
    "    import pystac_client as pc\n",
    "    import xarray as xr\n",
    "    import zarr\n",
    "    import numpy as np\n",
    "    import rioxarray\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    from collections import defaultdict\n",
    "    import yaml\n",
    "\n",
    "    def group_by_relative_orbit(items, key=\"sat:relative_orbit\"):\n",
    "        groups = defaultdict(list)\n",
    "        for it in items:\n",
    "            groups[it[0].properties[key]].append(it)\n",
    "        return dict(groups)\n",
    "\n",
    "    def read_and_pop_yaml(filename):\n",
    "        with open(filename) as f:\n",
    "            data = yaml.safe_load(f)\n",
    "        popped = data[\"ranges\"].pop(0)  # remove and get first range\n",
    "        with open(filename, \"w\") as f:\n",
    "            yaml.dump(data, f)\n",
    "        return popped\n",
    "\n",
    "    def get_idx(array1, array2):\n",
    "        min = np.where(array1==array2[0])[0][0]\n",
    "        max = np.where(array1==array2[-1])[0][0]+1\n",
    "        return min, max\n",
    "\n",
    "    def load_data(item, pols):\n",
    "        if type(pols)==str:\n",
    "            data = rioxarray.open_rasterio(item.assets[pols].href).compute().expand_dims(time=pd.to_datetime([item.properties[\"datetime\"]]).tz_convert(None)).rename(pols)\n",
    "        else:\n",
    "            data = []\n",
    "            for pol in pols:\n",
    "                data.append(rioxarray.open_rasterio(item.assets[pol].href).compute().expand_dims(time=pd.to_datetime([item.properties[\"datetime\"]]).tz_convert(None)).rename(pol))\n",
    "            \n",
    "            data = xr.merge(data)\n",
    "        return data.squeeze()\n",
    "\n",
    "    def get_datetime(item):\n",
    "        return datetime.strptime(item.properties[\"datetime\"], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    def group_dates(item_list):\n",
    "        grouped_items = [[]]\n",
    "        i=0\n",
    "        for item in item_list:\n",
    "            \n",
    "            if not grouped_items[i]:\n",
    "                grouped_items[i].append(item)\n",
    "            \n",
    "            else: \n",
    "                if get_datetime(item) - get_datetime(grouped_items[i][-1]) <= pd.Timedelta(seconds=100):\n",
    "                    grouped_items[i].append(item)\n",
    "\n",
    "                else:\n",
    "                    grouped_items.append([item])\n",
    "                    i+=1\n",
    "        return grouped_items\n",
    "\n",
    "    def read_and_merge_items(items, pols):\n",
    "        first = True\n",
    "        if type(pols)==list:\n",
    "            datasets = []\n",
    "            for pol in pols:\n",
    "                for item in items:\n",
    "                    ds = load_data(item, pol)\n",
    "                    \n",
    "                    if first:\n",
    "                        data = ds\n",
    "                        first = False\n",
    "                    \n",
    "                    else:\n",
    "                        data = xr.where(data==-9999, ds, data, keep_attrs=True)\n",
    "\n",
    "                if \"time\" in data.dims:      \n",
    "                    datasets.append(data)\n",
    "                else:\n",
    "                    datasets.append(data.expand_dims(time=pd.to_datetime([item.properties[\"datetime\"]]).tz_convert(None)))\n",
    "\n",
    "                first=True\n",
    "            data = xr.merge(datasets)\n",
    "\n",
    "        else:\n",
    "            for item in items:\n",
    "                ds = load_data(item, pols)\n",
    "                \n",
    "                if first:\n",
    "                    data = ds\n",
    "                    first = False\n",
    "                \n",
    "                else:\n",
    "                    data = xr.where(data==-9999, ds, data, keep_attrs=True)\n",
    "\n",
    "            data = data.to_dataset(name=pols)\n",
    "\n",
    "        return data.squeeze()\n",
    "\n",
    "    print(\"start\")\n",
    "    pc_client = pc.Client.open(\"https://stac.eodc.eu/api/v1\")\n",
    "    time_range = \"2025-01-01/2025-02-01\"#read_and_pop_yaml(\"/eodc/private/tempearth/s1sig0_timesteps.yaml\")\n",
    "    search = pc_client.search(\n",
    "        collections=[\"SENTINEL1_SIG0_20M\"],\n",
    "        datetime=time_range,\n",
    "        query={\"Equi7_TileID\": {\"eq\": f\"EU020M_{tile}T3\"}})\n",
    "\n",
    "    items_eodc = search.item_collection()\n",
    "    print(\"test print\")\n",
    "    if items_eodc:\n",
    "\n",
    "        item_list = list(items_eodc)[::-1]\n",
    "        grouped_items = group_dates(item_list)\n",
    "\n",
    "        store = zarr.storage.LocalStore(store_path)\n",
    "        group = zarr.group(store=store)\n",
    "        x_extent = group[\"x\"][:]\n",
    "        y_extent = group[\"y\"][:]\n",
    "        rel_orbit_extent = group[\"relative_orbit_number\"][:]\n",
    "\n",
    "        datasets = []\n",
    "    \n",
    "        sensing_origin = np.datetime64(\"2014-10-01T00:00:00\")\n",
    "\n",
    "        start = np.datetime64(time_range.split(\"/\", 1)[0].strip(), \"D\")\n",
    "        end = np.datetime64(time_range.split(\"/\", 1)[1].strip(), \"D\")\n",
    "        \n",
    "        print(\"test print\")\n",
    "        grouped_orbits = group_by_relative_orbit(grouped_items)\n",
    "\n",
    "        for orbit, items_orbits in grouped_orbits.items():\n",
    "            print(f\"{orbit} started\")\n",
    "            orbit_index = np.where(rel_orbit_extent==orbit)[0][0]\n",
    "            datasets_orbits = []\n",
    "\n",
    "            for items in items_orbits:\n",
    "                ds = read_and_merge_items(items, [\"VV\", \"VH\"])\n",
    "                \n",
    "                ds = ds.expand_dims({\"rel_orbit_number\": [ds.attrs[\"rel_orbit_number\"]]})\n",
    "                ds[\"sensing_date\"] = (ds['time'].values.astype(\"datetime64[s]\") - sensing_origin).astype(\"int64\")\n",
    "                ds[\"abs_orbit_number\"] = ds.attrs[\"abs_orbit_number\"]\n",
    "                \n",
    "                ds['time'] = ds['time'].astype('datetime64[D]')\n",
    "\n",
    "                datasets_orbits.append(ds)\n",
    "                ds = None\n",
    "\n",
    "            combined_orbits = xr.concat(datasets_orbits, dim=\"time\", combine_attrs=\"override\")\n",
    "            full_times = pd.date_range(start=start, end=end, freq='D')\n",
    "            result = combined_orbits.reindex(time=full_times, fill_value=-9999)\n",
    "            datasets.append(result_orbits)\n",
    "\n",
    "            \n",
    "\n",
    "            # combined = xr.concat(datasets, dim=\"rel_orbit_number\", combine_attrs=\"override\").sortby(\"rel_orbit_number\")\n",
    "            # result = combined.reindex(rel_orbit_number=rel_orbit_extent, fill_value=-9999)\n",
    "\n",
    "            sensing_dates = result[\"sensing_date\"].values.reshape(1,result.sizes[\"time\"],1,1)\n",
    "            abs_orbit_numbers = result[\"abs_orbit_number\"].values.reshape(1,result.sizes[\"time\"],1,1)\n",
    "\n",
    "            result[\"x\"] = result.x-10\n",
    "            result[\"y\"] = result.y+10\n",
    "\n",
    "            x_min, x_max = get_idx(x_extent, result[\"x\"].values)\n",
    "            y_min, y_max = get_idx(y_extent, result[\"y\"].values)\n",
    "\n",
    "            time_origin = np.datetime64(\"2014-10-01\")\n",
    "            time_min = (result.time.min().values.astype(\"datetime64[D]\") - time_origin).astype(\"int64\")\n",
    "            time_max = (result.time.max().values.astype(\"datetime64[D]\") - time_origin).astype(\"int64\")\n",
    "\n",
    "            group[\"VH\"][orbit_index,time_min:time_max, y_min:y_max, x_min:x_max] = result[\"VH\"].values\n",
    "            group[\"VV\"][orbit_index,time_min:time_max, y_min:y_max, x_min:x_max] = result[\"VV\"].values\n",
    "\n",
    "            sensing_dates = np.broadcast_to(sensing_dates, (1,time_max-time_min, y_max-y_min, x_max-x_min))\n",
    "            abs_orbit_numbers = np.broadcast_to(abs_orbit_numbers, (1,time_max-time_min, y_max-y_min, x_max-x_min))\n",
    "            rel_orbit_numbers = np.broadcast_to(rel_orbit_numbers, (1,time_max-time_min, y_max-y_min, x_max-x_min))\n",
    "\n",
    "            group[\"sensing_date\"][orbit_index,time_min:time_max, y_min:y_max, x_min:x_max] = sensing_dates\n",
    "            group[\"absolute_orbit_number\"][orbit_index,time_min:time_max, y_min:y_max, x_min:x_max] = abs_orbit_numbers\n",
    "            print(f\"{orbit} done\", time_min, time_max, y_min, y_max, x_min, x_max)\n",
    "        print(\"success\")\n",
    "\n",
    "    else:\n",
    "        print(\"no items in collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb9df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@script(volume_mounts=[models.VolumeMount(name=\"eodc-mount\", mount_path=\"/eodc\")])\n",
    "def test(tile: str, store_path: str = \"/eodc/private/tempearth/s1sig0.zarr\"):\n",
    "    print(\"start\")\n",
    "    import pystac_client as pc\n",
    "    import xarray as xr\n",
    "    import zarr\n",
    "    import numpy as np\n",
    "    import rioxarray\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "\n",
    "    def get_idx(array1, array2):\n",
    "        min = np.where(array1==array2[0])[0][0]\n",
    "        max = np.where(array1==array2[-1])[0][0]+1\n",
    "        return min, max\n",
    "\n",
    "    def load_data(item, pols):\n",
    "        if type(pols)==str:\n",
    "            data = rioxarray.open_rasterio(item.assets[pols].href).compute().expand_dims(time=pd.to_datetime([item.properties[\"datetime\"]]).tz_convert(None)).rename(pols)\n",
    "        else:\n",
    "            data = []\n",
    "            for pol in pols:\n",
    "                data.append(rioxarray.open_rasterio(item.assets[pol].href).compute().expand_dims(time=pd.to_datetime([item.properties[\"datetime\"]]).tz_convert(None)).rename(pol))\n",
    "            \n",
    "            data = xr.merge(data)\n",
    "        return data.squeeze()\n",
    "\n",
    "    def get_datetime(item):\n",
    "        return datetime.strptime(item.properties[\"datetime\"], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    def group_dates(item_list):\n",
    "        grouped_items = [[]]\n",
    "        i=0\n",
    "        for item in item_list:\n",
    "            \n",
    "            if not grouped_items[i]:\n",
    "                grouped_items[i].append(item)\n",
    "            \n",
    "            else: \n",
    "                if get_datetime(item) - get_datetime(grouped_items[i][-1]) <= pd.Timedelta(seconds=100):\n",
    "                    grouped_items[i].append(item)\n",
    "\n",
    "                else:\n",
    "                    grouped_items.append([item])\n",
    "                    i+=1\n",
    "        return grouped_items\n",
    "\n",
    "    def read_and_merge_items(items, pols):\n",
    "        first = True\n",
    "        if type(pols)==list:\n",
    "            datasets = []\n",
    "            for pol in pols:\n",
    "                for item in items:\n",
    "                    ds = load_data(item, pol)\n",
    "                    \n",
    "                    if first:\n",
    "                        data = ds\n",
    "                        first = False\n",
    "                    \n",
    "                    else:\n",
    "                        data = xr.where(data==-9999, ds, data, keep_attrs=True)\n",
    "\n",
    "                if \"time\" in data.dims:      \n",
    "                    datasets.append(data)\n",
    "                else:\n",
    "                    datasets.append(data.expand_dims(time=pd.to_datetime([item.properties[\"datetime\"]]).tz_convert(None)))\n",
    "\n",
    "                first=True\n",
    "            data = xr.merge(datasets)\n",
    "\n",
    "        else:\n",
    "            for item in items:\n",
    "                ds = load_data(item, pols)\n",
    "                \n",
    "                if first:\n",
    "                    data = ds\n",
    "                    first = False\n",
    "                \n",
    "                else:\n",
    "                    data = xr.where(data==-9999, ds, data, keep_attrs=True)\n",
    "\n",
    "            data = data.to_dataset(name=pols)\n",
    "\n",
    "        return data.squeeze()\n",
    "\n",
    "    print(\"start\")\n",
    "    pc_client = pc.Client.open(\"https://stac.eodc.eu/api/v1\")\n",
    "    time_range = \"2025-01-01/2025-02-01\"#read_and_pop_yaml(\"/eodc/private/tempearth/s1sig0_timesteps.yaml\")\n",
    "    search = pc_client.search(\n",
    "        collections=[\"SENTINEL1_SIG0_20M\"],\n",
    "        datetime=time_range,\n",
    "        query={\"Equi7_TileID\": {\"eq\": f\"EU020M_{tile}T3\"}})\n",
    "\n",
    "    items_eodc = search.item_collection()\n",
    "\n",
    "    if items_eodc:\n",
    "\n",
    "        item_list = list(items_eodc)[::-1]\n",
    "        grouped_items = group_dates(item_list)\n",
    "\n",
    "        store = zarr.storage.LocalStore(store_path)\n",
    "        group = zarr.group(store=store)\n",
    "        x_extent = group[\"x\"][:]\n",
    "        y_extent = group[\"y\"][:]\n",
    "        rel_orbit_extent = group[\"relative_orbit_number\"][:]\n",
    "\n",
    "        datasets = []\n",
    "        datasets_orbits = []\n",
    "        sensing_origin = np.datetime64(\"2014-10-01T00:00:00\")\n",
    "\n",
    "        start = np.datetime64(time_range.split(\"/\", 1)[0].strip(), \"D\")\n",
    "        end = np.datetime64(time_range.split(\"/\", 1)[1].strip(), \"D\")\n",
    "\n",
    "        for items in grouped_items:\n",
    "            ds = read_and_merge_items(items, [\"VV\", \"VH\"])\n",
    "            \n",
    "            ds[\"sensing_date\"] = (ds['time'].values.astype(\"datetime64[s]\") - sensing_origin).astype(\"int64\")\n",
    "            ds[\"abs_orbit_number\"] = ds.attrs[\"abs_orbit_number\"]\n",
    "            ds['time'] = ds['time'].astype('datetime64[D]')\n",
    "\n",
    "            datasets_orbits.append(ds)\n",
    "            ds = None\n",
    "\n",
    "        combined_orbits = xr.concat(datasets_orbits, dim=\"time\", combine_attrs=\"override\")\n",
    "        full_times = pd.date_range(start=start, end=end, freq='D')\n",
    "        result = combined_orbits.reindex(time=full_times, fill_value=-9999)\n",
    "\n",
    "\n",
    "        sensing_dates = result[\"sensing_date\"].values.reshape(result.sizes[\"time\"],1,1)\n",
    "        abs_orbit_numbers = result[\"abs_orbit_number\"].values.reshape(result.sizes[\"time\"],1,1)\n",
    "        #rel_orbit_numbers = result[\"rel_orbit_number\"].values.reshape(result.sizes[\"time\"],1,1)\n",
    "\n",
    "        result[\"x\"] = result.x-10\n",
    "        result[\"y\"] = result.y+10\n",
    "\n",
    "        x_min, x_max = get_idx(x_extent, result[\"x\"].values)\n",
    "        y_min, y_max = get_idx(y_extent, result[\"y\"].values)\n",
    "\n",
    "        time_origin = np.datetime64(\"2014-10-01\")\n",
    "        time_min = (result.time.min().values.astype(\"datetime64[D]\") - time_origin).astype(\"int64\")\n",
    "        time_max = (result.time.max().values.astype(\"datetime64[D]\") - time_origin).astype(\"int64\")+1\n",
    "\n",
    "        group[\"VH\"][time_min:time_max, y_min:y_max, x_min:x_max] = result[\"VH\"].values\n",
    "        group[\"VV\"][time_min:time_max, y_min:y_max, x_min:x_max] = result[\"VV\"].values\n",
    "\n",
    "        sensing_dates = np.broadcast_to(sensing_dates, (time_max-time_min, y_max-y_min, x_max-x_min))\n",
    "        abs_orbit_numbers = np.broadcast_to(abs_orbit_numbers, (time_max-time_min, y_max-y_min, x_max-x_min))\n",
    "        rel_orbit_numbers = np.broadcast_to(rel_orbit_numbers, (time_max-time_min, y_max-y_min, x_max-x_min))\n",
    "\n",
    "        group[\"sensing_date\"][time_min:time_max, y_min:y_max, x_min:x_max] = sensing_dates\n",
    "        group[\"absolute_orbit_number\"][time_min:time_max, y_min:y_max, x_min:x_max] = abs_orbit_numbers\n",
    "        print(time_min, time_max, y_min, y_max, x_min, x_max)\n",
    "        print(\"success\")\n",
    "\n",
    "    else:\n",
    "        print(\"no items in collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "24e92867",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Workflow(\n",
    "    generate_name=\"s1sig0-zarr-\",\n",
    "    volumes = nfs_volume,\n",
    "    security_context=security_context,\n",
    "    entrypoint=\"workflow\"\n",
    ") as w:\n",
    "    with DAG(name=\"workflow\"):\n",
    "        #ext = extend_time_dimension()\n",
    "        process1 = write_data(name=\"E045N015\", arguments={\"tile\":\"E045N015\"})\n",
    "        process2 = write_data(name=\"E048N015\", arguments={\"tile\":\"E048N015\"})\n",
    "        process3 = write_data(name=\"E051N015\", arguments={\"tile\":\"E051N015\"})\n",
    "        process4 = write_data(name=\"E048N012\", arguments={\"tile\":\"E048N012\"})\n",
    "        process5 = write_data(name=\"E051N012\", arguments={\"tile\":\"E051N012\"})\n",
    "\n",
    "        #ext >> \n",
    "        process1 >> process2 >> process3 >> process4 >> process5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dd05c70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Workflow(api_version=None, kind=None, metadata=ObjectMeta(annotations=None, cluster_name=None, creation_timestamp=Time(__root__=datetime.datetime(2025, 8, 14, 9, 20, 41, tzinfo=datetime.timezone.utc)), deletion_grace_period_seconds=None, deletion_timestamp=None, finalizers=None, generate_name='s1sig0-zarr-', generation=1, labels={'workflows.argoproj.io/creator': 'system-serviceaccount-default-jenkins'}, managed_fields=[ManagedFieldsEntry(api_version='argoproj.io/v1alpha1', fields_type='FieldsV1', fields_v1=FieldsV1(), manager='argo', operation='Update', subresource=None, time=Time(__root__=datetime.datetime(2025, 8, 14, 9, 20, 41, tzinfo=datetime.timezone.utc)))], name='s1sig0-zarr-ndt24', namespace='spartacus', owner_references=None, resource_version='42965838', self_link=None, uid='e9e8c15e-cbcc-4b55-b341-b42e4db08d62'), spec=WorkflowSpec(active_deadline_seconds=None, affinity=None, archive_logs=None, arguments=Arguments(artifacts=None, parameters=None), artifact_gc=None, artifact_repository_ref=None, automount_service_account_token=None, dns_config=None, dns_policy=None, entrypoint='workflow', executor=None, hooks=None, host_aliases=None, host_network=None, image_pull_secrets=None, metrics=None, node_selector=None, on_exit=None, parallelism=None, pod_disruption_budget=None, pod_gc=None, pod_metadata=None, pod_priority=None, pod_priority_class_name=None, pod_spec_patch=None, priority=None, retry_strategy=None, scheduler_name=None, security_context=PodSecurityContext(fs_group=None, fs_group_change_policy=None, run_as_group=59100, run_as_non_root=None, run_as_user=59100, se_linux_options=None, seccomp_profile=None, supplemental_groups=None, sysctls=None, windows_options=None), service_account_name=None, shutdown=None, suspend=None, synchronization=None, template_defaults=None, templates=[Template(active_deadline_seconds=None, affinity=None, archive_location=None, automount_service_account_token=None, container=None, container_set=None, daemon=None, dag=DAGTemplate(fail_fast=None, target=None, tasks=[DAGTask(arguments=Arguments(artifacts=None, parameters=[Parameter(default=None, description=None, enum=None, global_name=None, name='tile', value='E045N015', value_from=None)]), continue_on=None, dependencies=None, depends=None, hooks=None, inline=None, name='E045N015', on_exit=None, template='write-data', template_ref=None, when=None, with_items=None, with_param=None, with_sequence=None), DAGTask(arguments=Arguments(artifacts=None, parameters=[Parameter(default=None, description=None, enum=None, global_name=None, name='tile', value='E048N015', value_from=None)]), continue_on=None, dependencies=None, depends='E045N015', hooks=None, inline=None, name='E048N015', on_exit=None, template='write-data', template_ref=None, when=None, with_items=None, with_param=None, with_sequence=None), DAGTask(arguments=Arguments(artifacts=None, parameters=[Parameter(default=None, description=None, enum=None, global_name=None, name='tile', value='E051N015', value_from=None)]), continue_on=None, dependencies=None, depends='E048N015', hooks=None, inline=None, name='E051N015', on_exit=None, template='write-data', template_ref=None, when=None, with_items=None, with_param=None, with_sequence=None), DAGTask(arguments=Arguments(artifacts=None, parameters=[Parameter(default=None, description=None, enum=None, global_name=None, name='tile', value='E048N012', value_from=None)]), continue_on=None, dependencies=None, depends='E051N015', hooks=None, inline=None, name='E048N012', on_exit=None, template='write-data', template_ref=None, when=None, with_items=None, with_param=None, with_sequence=None), DAGTask(arguments=Arguments(artifacts=None, parameters=[Parameter(default=None, description=None, enum=None, global_name=None, name='tile', value='E051N012', value_from=None)]), continue_on=None, dependencies=None, depends='E048N012', hooks=None, inline=None, name='E051N012', on_exit=None, template='write-data', template_ref=None, when=None, with_items=None, with_param=None, with_sequence=None)]), data=None, executor=None, fail_fast=None, host_aliases=None, http=None, init_containers=None, inputs=Inputs(artifacts=None, parameters=None), memoize=None, metadata=Metadata(annotations=None, labels=None), metrics=None, name='workflow', node_selector=None, outputs=Outputs(artifacts=None, exit_code=None, parameters=None, result=None), parallelism=None, plugin=None, pod_spec_patch=None, priority=None, priority_class_name=None, resource=None, retry_strategy=None, scheduler_name=None, script=None, security_context=None, service_account_name=None, sidecars=None, steps=None, suspend=None, synchronization=None, timeout=None, tolerations=None, volumes=None), Template(active_deadline_seconds=None, affinity=None, archive_location=None, automount_service_account_token=None, container=None, container_set=None, daemon=None, dag=None, data=None, executor=None, fail_fast=None, host_aliases=None, http=None, init_containers=None, inputs=Inputs(artifacts=None, parameters=[Parameter(default=None, description=None, enum=None, global_name=None, name='tile', value=None, value_from=None), Parameter(default='/eodc/private/tempearth/s1sig0.zarr', description=None, enum=None, global_name=None, name='store_path', value=None, value_from=None)]), memoize=None, metadata=Metadata(annotations=None, labels=None), metrics=None, name='write-data', node_selector=None, outputs=Outputs(artifacts=None, exit_code=None, parameters=None, result=None), parallelism=None, plugin=None, pod_spec_patch=None, priority=None, priority_class_name=None, resource=None, retry_strategy=None, scheduler_name=None, script=ScriptTemplate(args=None, command=['python'], env=None, env_from=None, image='ghcr.io/oscipal/image_zarr:latest', image_pull_policy=None, lifecycle=None, liveness_probe=None, name='', ports=None, readiness_probe=None, resources=ResourceRequirements(limits=None, requests=None), security_context=None, source=\"import os\\nimport sys\\nsys.path.append(os.getcwd())\\nimport json\\ntry: store_path = json.loads(r'''{{inputs.parameters.store_path}}''')\\nexcept: store_path = r'''{{inputs.parameters.store_path}}'''\\ntry: tile = json.loads(r'''{{inputs.parameters.tile}}''')\\nexcept: tile = r'''{{inputs.parameters.tile}}'''\\n\\nprint('test1')\\nimport pystac_client as pc\\nimport xarray as xr\\nimport zarr\\nimport numpy as np\\nimport rioxarray\\nimport pandas as pd\\nfrom datetime import datetime\\nfrom collections import defaultdict\\nimport yaml\\n\\ndef group_by_relative_orbit(items, key='sat:relative_orbit'):\\n    groups = defaultdict(list)\\n    for it in items:\\n        groups[it[0].properties[key]].append(it)\\n    return dict(groups)\\n\\ndef read_and_pop_yaml(filename):\\n    with open(filename) as f:\\n        data = yaml.safe_load(f)\\n    popped = data['ranges'].pop(0)\\n    with open(filename, 'w') as f:\\n        yaml.dump(data, f)\\n    return popped\\n\\ndef get_idx(array1, array2):\\n    min = np.where(array1 == array2[0])[0][0]\\n    max = np.where(array1 == array2[-1])[0][0] + 1\\n    return (min, max)\\n\\ndef load_data(item, pols):\\n    if type(pols) == str:\\n        data = rioxarray.open_rasterio(item.assets[pols].href).compute().expand_dims(time=pd.to_datetime([item.properties['datetime']]).tz_convert(None)).rename(pols)\\n    else:\\n        data = []\\n        for pol in pols:\\n            data.append(rioxarray.open_rasterio(item.assets[pol].href).compute().expand_dims(time=pd.to_datetime([item.properties['datetime']]).tz_convert(None)).rename(pol))\\n        data = xr.merge(data)\\n    return data.squeeze()\\n\\ndef get_datetime(item):\\n    return datetime.strptime(item.properties['datetime'], '%Y-%m-%dT%H:%M:%SZ')\\n\\ndef group_dates(item_list):\\n    grouped_items = [[]]\\n    i = 0\\n    for item in item_list:\\n        if not grouped_items[i]:\\n            grouped_items[i].append(item)\\n        elif get_datetime(item) - get_datetime(grouped_items[i][-1]) <= pd.Timedelta(seconds=100):\\n            grouped_items[i].append(item)\\n        else:\\n            grouped_items.append([item])\\n            i += 1\\n    return grouped_items\\n\\ndef read_and_merge_items(items, pols):\\n    first = True\\n    if type(pols) == list:\\n        datasets = []\\n        for pol in pols:\\n            for item in items:\\n                ds = load_data(item, pol)\\n                if first:\\n                    data = ds\\n                    first = False\\n                else:\\n                    data = xr.where(data == -9999, ds, data, keep_attrs=True)\\n            if 'time' in data.dims:\\n                datasets.append(data)\\n            else:\\n                datasets.append(data.expand_dims(time=pd.to_datetime([item.properties['datetime']]).tz_convert(None)))\\n            first = True\\n        data = xr.merge(datasets)\\n    else:\\n        for item in items:\\n            ds = load_data(item, pols)\\n            if first:\\n                data = ds\\n                first = False\\n            else:\\n                data = xr.where(data == -9999, ds, data, keep_attrs=True)\\n        data = data.to_dataset(name=pols)\\n    return data.squeeze()\\nprint('start')\\npc_client = pc.Client.open('https://stac.eodc.eu/api/v1')\\ntime_range = '2025-01-01/2025-02-01'\\nsearch = pc_client.search(collections=['SENTINEL1_SIG0_20M'], datetime=time_range, query={'Equi7_TileID': {'eq': f'EU020M_{tile}T3'}})\\nitems_eodc = search.item_collection()\\nprint('test print')\\nif items_eodc:\\n    item_list = list(items_eodc)[::-1]\\n    grouped_items = group_dates(item_list)\\n    store = zarr.storage.LocalStore(store_path)\\n    group = zarr.group(store=store)\\n    x_extent = group['x'][:]\\n    y_extent = group['y'][:]\\n    rel_orbit_extent = group['relative_orbit_number'][:]\\n    datasets = []\\n    sensing_origin = np.datetime64('2014-10-01T00:00:00')\\n    start = np.datetime64(time_range.split('/', 1)[0].strip(), 'D')\\n    end = np.datetime64(time_range.split('/', 1)[1].strip(), 'D')\\n    print('test print')\\n    grouped_orbits = group_by_relative_orbit(grouped_items)\\n    for orbit, items_orbits in grouped_orbits.items():\\n        print(f'{orbit} started')\\n        datasets_orbits = []\\n        for items in items_orbits:\\n            ds = read_and_merge_items(items, ['VV', 'VH'])\\n            ds = ds.expand_dims({'rel_orbit_number': [ds.attrs['rel_orbit_number']]})\\n            ds['sensing_date'] = (ds['time'].values.astype('datetime64[s]') - sensing_origin).astype('int64')\\n            ds['abs_orbit_number'] = ds.attrs['abs_orbit_number']\\n            ds['time'] = ds['time'].astype('datetime64[D]')\\n            datasets_orbits.append(ds)\\n            ds = None\\n        combined_orbits = xr.concat(datasets_orbits, dim='time', combine_attrs='override')\\n        full_times = pd.date_range(start=start, end=end, freq='D')\\n        result_orbits = combined_orbits.reindex(time=full_times, fill_value=-9999)\\n        datasets.append(result_orbits)\\n        print(f'{orbit} done')\\n    combined = xr.concat(datasets, dim='rel_orbit_number', combine_attrs='override').sortby('rel_orbit_number')\\n    result = combined.reindex(rel_orbit_number=rel_orbit_extent, fill_value=-9999)\\n    sensing_dates = result['sensing_date'].values.reshape(8, result.sizes['time'], 1, 1)\\n    abs_orbit_numbers = result['abs_orbit_number'].values.reshape(8, result.sizes['time'], 1, 1)\\n    result['x'] = result.x - 10\\n    result['y'] = result.y + 10\\n    x_min, x_max = get_idx(x_extent, result['x'].values)\\n    y_min, y_max = get_idx(y_extent, result['y'].values)\\n    time_origin = np.datetime64('2014-10-01')\\n    time_min = (result.time.min().values.astype('datetime64[D]') - time_origin).astype('int64')\\n    time_max = (result.time.max().values.astype('datetime64[D]') - time_origin).astype('int64') + 1\\n    group['VH'][:, time_min:time_max, y_min:y_max, x_min:x_max] = result['VH'].values\\n    group['VV'][:, time_min:time_max, y_min:y_max, x_min:x_max] = result['VV'].values\\n    sensing_dates = np.broadcast_to(sensing_dates, (8, time_max - time_min, y_max - y_min, x_max - x_min))\\n    abs_orbit_numbers = np.broadcast_to(abs_orbit_numbers, (8, time_max - time_min, y_max - y_min, x_max - x_min))\\n    rel_orbit_numbers = np.broadcast_to(rel_orbit_numbers, (8, time_max - time_min, y_max - y_min, x_max - x_min))\\n    group['sensing_date'][:, time_min:time_max, y_min:y_max, x_min:x_max] = sensing_dates\\n    group['absolute_orbit_number'][:, time_min:time_max, y_min:y_max, x_min:x_max] = abs_orbit_numbers\\n    print(time_min, time_max, y_min, y_max, x_min, x_max)\\n    print('success')\\nelse:\\n    print('no items in collection')\", startup_probe=None, stdin=None, stdin_once=None, termination_message_path=None, termination_message_policy=None, tty=None, volume_devices=None, volume_mounts=[VolumeMount(mount_path='/eodc', mount_propagation=None, name='eodc-mount', read_only=None, sub_path=None, sub_path_expr=None)], working_dir=None), security_context=None, service_account_name=None, sidecars=None, steps=None, suspend=None, synchronization=None, timeout=None, tolerations=None, volumes=None)], tolerations=None, ttl_strategy=None, volume_claim_gc=None, volume_claim_templates=None, volumes=[Volume(aws_elastic_block_store=None, azure_disk=None, azure_file=None, cephfs=None, cinder=None, config_map=None, csi=None, downward_api=None, empty_dir=None, ephemeral=None, fc=None, flex_volume=None, flocker=None, gce_persistent_disk=None, git_repo=None, glusterfs=None, host_path=None, iscsi=None, name='eodc-mount', nfs=None, persistent_volume_claim=PersistentVolumeClaimVolumeSource(claim_name='eodc-nfs-claim', read_only=None), photon_persistent_disk=None, portworx_volume=None, projected=None, quobyte=None, rbd=None, scale_io=None, secret=None, storageos=None, vsphere_volume=None)], workflow_metadata=None, workflow_template_ref=None), status=WorkflowStatus(artifact_gc_status=None, artifact_repository_ref=None, compressed_nodes=None, conditions=None, estimated_duration=None, finished_at=None, message=None, nodes=None, offload_node_status_version=None, outputs=None, persistent_volume_claims=None, phase=None, progress=None, resources_duration=None, started_at=None, stored_templates=None, stored_workflow_template_spec=None, synchronization=None, task_results_completion_status=None))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8265fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zarr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

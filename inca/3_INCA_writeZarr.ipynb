{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f13bf8f",
   "metadata": {},
   "source": [
    "### Writing data to an existing zarr store (INCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a417dda6",
   "metadata": {},
   "source": [
    "Writing data to an exisiting store should be done automatically with [argo worklfows](ArgoWorkflows.ipynb), this notebook describes how it is done manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deac652",
   "metadata": {},
   "source": [
    "#### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a61a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import zarr\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb41df65",
   "metadata": {},
   "source": [
    "This function will help in a later step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab05bfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx(array1, array2):\n",
    "    min = np.where(array1==array2[0])[0][0]\n",
    "    max = np.where(array1==array2[-1])[0][0]+1\n",
    "    return min, max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc074fd6",
   "metadata": {},
   "source": [
    "#### Writing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b090fff6",
   "metadata": {},
   "source": [
    "Define the path to your store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20da396",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_path = \"INCA.zarr\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36063d67",
   "metadata": {},
   "source": [
    "In a [previous step](INCA_download.ipynb) the necessary data is downloaded from the [geosphere data hub](https://data.hub.geosphere.at/dataset/inca-v1-1h-1km) and saved to the folder INCA_data. The writing is done individually for each available parameter in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1943d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "param =\"T2M\"\n",
    "\n",
    "# Get the paths to each file\n",
    "folder_path = f'INCA_data/{param}'\n",
    "filepaths = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        filepaths.append(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b89e05",
   "metadata": {},
   "source": [
    "The writing is done by looping through the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce01bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the dataarray and get metadata parameters for writing. \n",
    "store = zarr.storage.LocalStore(store_path)\n",
    "group = zarr.group(store=store)\n",
    "\n",
    "dtype = group[param].dtype\n",
    "fill_value = group[param].attrs.get('_FillValue')\n",
    "freq = group.attrs.get('freq')\n",
    "\n",
    "# Get the x and y extent of the group\n",
    "x_extent = group[\"x\"][:]\n",
    "y_extent = group[\"y\"][:]\n",
    "\n",
    "# Get the origin time value from the zarr store\n",
    "origin = xr.open_zarr(store_path).time[0].values\n",
    "\n",
    "# Loop through the files\n",
    "for i, file in enumerate(filepaths):\n",
    "    \n",
    "    # Reading the data\n",
    "    data = xr.open_dataset(file, chunks={}, mask_and_scale=False) # Set mask_and_scale to Fales to get the raw data values\n",
    "    data = data.load()\n",
    "\n",
    "    # If the loaded data has a smaller spatial exent than the whole extent of the zarr store, you need to know the indexes where to write the data.\n",
    "    x_min, x_max = get_idx(x_extent, data[\"x\"].values)\n",
    "    y_min, y_max = get_idx(y_extent, data[\"y\"].values)\n",
    "\n",
    "    # To write in the correct time coordinates you need the starting and end time of your data\n",
    "    time_min, time_max = data.time.values[0].astype(\"datetime64[h]\"), data.time.values[-1].astype(\"datetime64[h]\")+1\n",
    "    \n",
    "    # And  the corresponding indexes in the zarr store originating from the origin\n",
    "    time_delta_min, time_delta_max = (time_min - origin).astype(\"int64\"), (time_max - origin).astype(\"int64\")\n",
    "\n",
    "    ### If you have a missing timestep in your data this snippet will add the timestep filled with FillValue ###\n",
    "    \n",
    "    # First create a range where all timesteps are present\n",
    "    full_range = pd.date_range(time_min, time_max, freq=freq).values.astype(\"datetime64[ns]\")\n",
    "    \n",
    "    # Check if the data has no missing timesteps\n",
    "    for value in data.time.values:\n",
    "        if value in set(full_range):\n",
    "            continue\n",
    "        else:\n",
    "            # If there are missing timesteps, a dataset is created with all timesteps and only FillValues\n",
    "            print(f\"{file} Data incomplete\")\n",
    "            empty_array = np.full((full_range.shape[0], data[\"x\"].values.shape[0], data[\"y\"].values.shape[0]),\n",
    "                                fill_value=fill_value, dtype=dtype)\n",
    "\n",
    "            template = xr.Dataset({f\"{param}\": ((\"time\", \"x\", \"y\"), empty_array)},\n",
    "                                  coords={\n",
    "                                    \"time\": full_range,\n",
    "                                    \"x\": data[\"x\"].values,\n",
    "                                    \"y\": data[\"y\"].values\n",
    "                                  }\n",
    "                                  )\n",
    "\n",
    "            # The data is then combined to form a dataset with all timesteps present\n",
    "            data = data.combine_first(template)\n",
    "            print(f\"{file} Data gaps filled with no data values\")\n",
    "            break\n",
    "    ### end of snippet ###\n",
    "\n",
    "    # Write the data in the zarr data array, with the correct indices in spatial  and temporal domain\n",
    "    group[param][time_delta_min:time_delta_max, y_min:y_max, x_min:x_max] = data[param].values\n",
    "\n",
    "    print(f\"{file} written to zarr store. {i}/{len(filepaths)} completeðŸ’Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9318889d",
   "metadata": {},
   "source": [
    "### Inspecting the zarr store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75df8d3",
   "metadata": {},
   "source": [
    "After writing to the store you should check if the data in the store matches the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e31dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = xr.open_dataset(filepaths[0]).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a2d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_min = original_data.time.values[0]\n",
    "time_max = original_data.time.values[-1]\n",
    "zarr_data = xr.open_zarr(store_path)[param].sel(time=slice(time_min, time_max)).load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zarr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
